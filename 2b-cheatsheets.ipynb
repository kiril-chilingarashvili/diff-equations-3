{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10491037-0a41-4d77-a666-6ab21d03cc9d",
   "metadata": {},
   "source": [
    "# Unit 2: Linear Algebra, Part 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2039ec9-cee4-42b4-880b-67fd08ad33af",
   "metadata": {},
   "source": [
    "## The eigenvalue-eigenvector problem\n",
    "\n",
    "### In the course Differential equations: 2 by 2 systems, we learned that the first step in solving a linear $2 \\times 2$ system of differential equations, $\\dot{\\bf{x}}$ is to find the eigenvalues and eigenvectors of the $2 \\times 2$ matrix $\\bf{A}$. The procedure for solving linear $n \\times n$ systems of DEs is the same, and starts with finding eigenvalues and eigenvectors. And even outside the context of differential equations, the eigenvalues and eigenvectors of a matrix tell us a lot about what the matrix does as a function on $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29d916-661b-4e36-99ce-11338cc64d4b",
   "metadata": {},
   "source": [
    "### ***Definition 2.1***\n",
    "### Let $\\bf{A}$ be an $n \\times n$ mmatrix.\n",
    "### - An ***eigenvalue of*** $\\bf{A}$ is a ***scalar*** $\\lambda$ such that $\\bf{A}\\bf{v} = \\lambda \\bf{v}$ for some ***nonzero*** vector $\\bf{v}$.\n",
    "### - An ***eigenvector of*** $\\bf{A}$ ***associated with an eigenvalue*** $\\lambda$ is a ***nonzero*** vector $\\bf{v}$ such that $\\bf{A}\\bf{v} = \\lambda \\bf{v}$.\n",
    "\n",
    "### (We also say that an eigenvector $\\bf{v}$ “corresponds to,\" or “belongs to\" an eigenvalue $\\lambda$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7c22-b535-4718-a698-b3e6d9ca3254",
   "metadata": {},
   "source": [
    "### The eigenvalue-eigenvector problem is to find all possible scalars $\\lambda$ and for each $\\lambda$ all vectors $\\bf{v}$ such that\n",
    "## $$ \\bf{A}\\bf{v} = \\lambda \\bf{v} $$\n",
    "\n",
    "### ***Warning***: Eigenvalues and eigenvectors are defined only for ***square*** matrices.\n",
    "### ***Warning***: The zero vector $\\bf{0}$ is never an eigenvector.\n",
    "### ***Note***: Everyone allows that $\\lambda = 0$ can be an eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71228bca-50d5-4811-8694-0d7cc7cbc856",
   "metadata": {},
   "source": [
    "#### ***Example 2.2***\n",
    "### The matrix $\\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$ satisfies\n",
    "## $$ \\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\bf{v} = 5 \\bf{v} \\quad \\text{for all}\\, \\bf{v}\\,\\text{in}\\,\\mathbb{R}^3 $$\n",
    "\n",
    "### Therefore, the number $5$ is an eigenvalue and all (nonzero) vectors in $\\mathbb{R}^3$ are eigenvectors associated to the eigenvalue $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204383b4-72b2-485a-bd14-bdbcd2a41382",
   "metadata": {},
   "source": [
    "### ***Example 2.3***\n",
    "### The diagonal matrix $\\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}$ satisfies the following:\n",
    "## $$ \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n",
    "## $$ \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n",
    "## $$ \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = -1 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n",
    "### The first equation above shows that the scalar $2$ is eigenvalue with associated eigenvector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n",
    "### Similarly, the second and third equations show that $0$ and $-1$ are both eigenvalues, and $\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$ are eigenvectors associated to $0$ and $-1$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77905f-2ed9-43b7-a996-d5b681a06d5e",
   "metadata": {},
   "source": [
    "### ***Example 2.4***\n",
    "### The matrix $\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$ satisfies\n",
    "## $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}  $$\n",
    "## $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = (-1)\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n",
    "### Therefore, the scalar $1$ is an eigenvalue with an associated eigenvector $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and $-1$ is an eigenvalue with an associated eigenvector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5875f-50c0-4311-a337-be29e4e6abe4",
   "metadata": {},
   "source": [
    "## Geometric meaning\n",
    "\n",
    "### ***Geometric meaning of real eigenvalues and eigenvectors***\n",
    "### Recall that any $n \\times n$ matrix $\\bf{A}$ represents a function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. Therefore, an eigenvector $\\bf{v}$ of $\\bf{A}$ which satisfies the equation\n",
    "## $$ \\bf{A}\\bf{v} = \\lambda \\bf{v} \\quad \\text{for some scalar}\\, \\lambda $$\n",
    "### is a vector whose image under $\\bf{A}$ is a scalar multiple of itself. When the eigenvalue $\\lambda$ is real, this means that an eigenvector is a vector  whose image lies on the line in $\\mathbb{R}^n$ through $\\bf{0}$ and $\\bf{v}$, with the eigenvalue $\\lambda$ as the scaling factor.\n",
    "\n",
    "### Let us revisit the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01015c1-653d-448d-a64d-79aeacfbd129",
   "metadata": {},
   "source": [
    "### ***Example 3.1***\n",
    "### The function represented by $\\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$ stretches every vector in $\\mathbb{R}^3$ to $5$ times it length but does not change its direction; hence, every vector is an eigenvector associated to the eigenvalue $5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8fd5a-9e3e-4051-b0bf-0ee0cac243ca",
   "metadata": {},
   "source": [
    "### ***Example 3.2***\n",
    "### The function represented by the matrix $\\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}$\n",
    "### - stretches the eigenvector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ to $2$ times ($2$ is the corresponding eigenvalue) its length but does not change its direction;\n",
    "### - collapses the eigenvector $\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$to the $\\bf{0}$ vector, since $0$ is the corresponding eigenvalue;\n",
    "### - flips the eigenvector $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$ across the origin to the other side of the $z$-axis without changing its length, since $-1$ is the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965cb1e-0813-4477-b807-ca35ef644960",
   "metadata": {},
   "source": [
    "### ***Example 3.3***\n",
    "### The (function represented by the) matrix $\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$,\n",
    "### - does not change the direction or length of the eigenvector $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ since the corresponding eigenvalue is $1$;\n",
    "### - flips the eigenvector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ across the origin to the other side of the line $y=-x$, since the eigenvalue is $-1$.\n",
    "### In each case, the image of the eigenvector lies on the same line as the eigenvector itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8eb96-fe06-44c3-853f-90def4dacca3",
   "metadata": {},
   "source": [
    "## Matrix vector mathlet\n",
    "### The mathlet below shows the input and output vectors of a $2 \\times 2$ matrix $\\bf{A}$ and their relationship with the eigenlines. Recall from the course Differential equations: 2 by 2 systems that when the eigenvectors of an eigenvalue are all scalar mulitiples of just one eigenvector, then the line consisting of all the eigenvectors is called an ***eigenline***.\n",
    "### To see the action of $\\bf{A}$ on its eigenvector:\n",
    "### 1. click on all three boxes “Show eigenlines,\" “Show eigenvalues,\" and “Show eigenvectors\";\n",
    "### 2. choose values of $\\bf{A}$ (on the right) so that two eigenlines (green) are shown on the graph (on the left);\n",
    "### 3. click on a point along the eigenlines (green on the graph) to select an eigenvector $\\bf{v}$ as an input to $\\bf{A}$;\n",
    "### 4. observe that the output $\\bf{A}\\bf{v}$ lies along the same line as $\\bf{v}$ with the corresponding eigenvalue, $\\lambda_1$ or $\\lambda_2$ (bottom right) as scalar factors.\n",
    "### Observe that when the input vector $\\bf{v}$ is not an eigenvector (i.e. not on an eigenline), the output $\\bf{A}\\bf{v}$ is not on the same line as $\\bf{v}$.\n",
    "\n",
    "### [Mathlet link](https://mathlets.org/mathlets/matrix-vector/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9a8e6-1a6a-4605-86fa-483108412ee9",
   "metadata": {},
   "source": [
    "## Characteristic polynomial\n",
    "\n",
    "### The ***characteristic polynomial*** of an $n \\times n$ matrix $\\bf{A}$ is defined as\n",
    "## $$ P(\\lambda) \\colon = \\det (\\lambda \\bf{I} - \\bf{A}) $$\n",
    "\n",
    "### (If instead you use $\\det (\\bf{A} - \\lambda \\bf{I})$, you will need to negate the result when $n$ is odd in order to get a polynomial that starts with $\\lambda^n$ instead of $-\\lambda^n$.)\n",
    "\n",
    "### The roots of $P(\\lambda)$ are the eigenvalues of $\\bf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e0b0a-9c06-4f24-9697-b27b0ba07e34",
   "metadata": {},
   "source": [
    "### If $\\lambda$ is an eigenvalue, then there is a nonzero vector $\\bf{v}$ so that $\\bf{A}\\bf{v} = \\lambda \\bf{v}$. Rewriting the equation we get\n",
    "## $$ \\begin{array} {rcl} \\bf{A}\\bf{v} & = & \\lambda \\bf{v} \\\\ \\bf{A} \\bf{v} & = & \\lambda \\bf{I} \\bf{v} \\quad \\text{where }\\, \\bf{I}\\,\\text{is the identity matrix} \\\\ \\bf{0} & = & \\lambda\\bf{I}\\bf{v}-\\bf{A}\\bf{v} \\\\ \\bf{0} & = & (\\lambda\\bf{I} - \\bf{A}) \\bf{v} \\end{array} $$\n",
    "### This means that for a fixed $\\lambda$, the set of al vectors $\\bf{v}$ satisfying $(\\lambda\\bf{I}-\\bf{A})\\bf{v} = 0$ is the set of vectors satisfying $\\bf{A}\\bf{v} = \\lambda\\bf{v}$\n",
    "\n",
    "### Therefore $\\lambda$ is an eigenvalue if and only if the nullspace of the matrix $\\lambda\\bf{I}-\\bf{A}$ is nontrivial. And the nullspace is nontrivial if and only if $\\det (\\lambda\\bf{I}-\\bf{A}) = 0$. (This is an application of the theorem from the last lecture.) Therefore $\\lambda$ is an eigenvalue if and only if $\\det (\\lambda\\bf{I}-\\bf{A}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a906f8b-21db-43f3-b66d-586a2e5965d0",
   "metadata": {},
   "source": [
    "### ***Problem 4.1***\n",
    "### What are the eigenvalues of the matrix\n",
    "## $$ \\bf{A} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\, \\text{?} $$\n",
    "\n",
    "### ***Solution***\n",
    "### The characteristic polynomial of $\\bf{A}$ is\n",
    "## $$ \\begin{array} {rcl} \\det (\\lambda\\bf{I}-\\bf{A}) & = & \\left| \\begin{array} {ccc} \\lambda-1 & 0 & -1 \\\\ 0 & \\lambda-1 & 0 \\\\ -1 & 0 & \\lambda-1 \\end{array} \\right| \\\\ \\, & = & \\underbrace{(\\lambda-1)((\\lambda-1)(\\lambda-1)-(0)(0))+(-1)((0)(0)-(-1)(\\lambda-1))}_{\\text{expansion along the top row}} \\\\ \\, & = & \\lambda^3-3\\lambda^2+2\\lambda \\\\ \\, & = & \\lambda(1-\\lambda)(2-\\lambda) \\end{array} $$\n",
    "\n",
    "### The roots of the characteristic polynomial are $0,1,2$ and therefore these are the eigenvalues of $\\bf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf70991-9715-4ca0-8485-3b0d292ab579",
   "metadata": {},
   "source": [
    "### ***Definition 4.2***\n",
    "\n",
    "### The ***multiplicity*** of an eigenvalue $\\lambda$ is its multiplicity as a root of the characteristic polynomial.\n",
    "### In example above, each of the eigenvalues $0,1$, and $2$ has multiplicity $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7d9a9-c05e-41cf-8859-4f75aae701aa",
   "metadata": {},
   "source": [
    "### ***Example 4.3***\n",
    "### For the matrix $\\bf{A} = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$, find the eigenvalues and their multiplicities.\n",
    "\n",
    "### ***Solution***:\n",
    "### The characteristic polynomial is:\n",
    "## $$ \\begin{array} {rcl} \\det (\\lambda \\bf{I} - \\bf{A}) & = & \\left| \\begin{array} {ccc} \\lambda+2 & -1 & -1 \\\\ -1 & \\lambda+2 & -1 \\\\ -1 & -1 & \\lambda+2 \\end{array} \\right| \\\\ \\, & = & (\\lambda+2)((\\lambda+2)(\\lambda+2)-1)-(-1)((-1)(\\lambda+2)-(-1)(-1))+(-1)((-1)(-1)-(-1)(\\lambda+2)) \\\\ \\, & = & \\lambda^3+6\\lambda^2+9\\lambda \\\\ \\, & = & \\lambda(\\lambda+3)^2 \\end{array} $$\n",
    "### Therefore, the eigenvalues are $0$, with multiplicity $1$, and $-3$, with multiplicity $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274e13e-9d7c-4d37-b1e0-e4c5ba71464f",
   "metadata": {},
   "source": [
    "## Complex eigenvalues\n",
    "\n",
    "### Since eigenvalues are roots of a polynomial with real coefficients, they can be complex, and when they are, they come in pairs of complex conjugates.\n",
    "\n",
    "### ***Example 5.1***\n",
    "### Find the eigenvalues of the matrix $\\bf{A} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n",
    "### (This matrix represents the function that rotates any vector in $\\mathbb{R}^3$ by $90^\\circ$ counterclockwise about the $z$-axis.)\n",
    "\n",
    "### ***Solution***:\n",
    "## $$ \\begin{array} {rcl} \\det (\\bf{A} - \\lambda\\bf{I}) & = & \\left| \\begin{array} {rcl} -\\lambda & -1 & 0 \\\\ 1 & -\\lambda & 0 \\\\ 0 & 0 & 1-\\lambda \\end{array} \\right| \\\\ \\, & = & (\\lambda^2+1)(1-\\lambda) \\\\ \\, & = & -(\\lambda + i)(\\lambda-i)(\\lambda-1) \\\\ \\implies \\lambda & = & \\pm i,1 \\end{array} $$\n",
    "\n",
    "### Hence, the eigenvalues are $\\lambda=\\pm i, 1$, (each with multiplicity $1$). Note that both $i$ and its conjugate $-i$ are eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66144afb-81c2-4442-a6aa-e17724423360",
   "metadata": {},
   "source": [
    "### ***Theorem 5.2***\n",
    "### For any $n \\times n$ matrix, the total number of complex eigenvalues, counted with multiplicity, is $n$.\n",
    "### Note that complex eigenvalues include real eigenvalues.\n",
    "\n",
    "### ***Proof***\n",
    "### We apply the fundamental theorem of algebra to the characteristic polynomial of $\\bf{A}$, which is a degree $n$ polynomial with real coefficients.\n",
    "\n",
    "### ***The fundamental theorem of algebra***:  Every nonzero degree $n$ polynomial with complex coefficients has exactly $n$ complex roots when counted with multiplicity.\n",
    "\n",
    "### Remember real numbers are also complex numbers, so this implies in particular that every nonzero degree $n$ polynomial with real coefficients, e.g. the characteristic polynomial of $\\bf{A}$ has exactly $n$ complex roots when counted with multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edfffd-9dbc-4354-92be-f1f18d478680",
   "metadata": {},
   "source": [
    "## Eigenspaces\n",
    "\n",
    "### We now know that the eigenvalues of a matrix are the roots of the characteristic equation $\\det (\\lambda\\bf{I} - \\bf{A}) = 0$. Let us proceed to find the eigenvectors corresponding to each eigenvalue.\n",
    "\n",
    "### ***Definition 6.1***\n",
    "### The ***eigenspace*** of an eigenvalue $\\lambda$ of a square matrix $\\bf{A}$ is the set of all eigenvectors corresponding to $\\lambda$ together with the zero vector $\\bf{0}$. In other words, it is the span of all the eigenvectors associated to the eigenvalue $\\lambda$.\n",
    "### In other words,\n",
    "## $$ \\begin{array} {rcl} \\text{the eigenspace of an eigenvalue}\\,\\lambda & = & \\{\\text{span of all eigenvectors associated to}\\,\\lambda\\} \\\\ \\, & = & \\{ \\text{all solutions to}\\, (\\lambda\\bf{I}-\\bf{A})\\bf{v} = \\bf{0} \\} \\\\ \\, & = & \\operatorname{NS}(\\lambda\\bf{I}-\\bf{A}) \\end{array} $$\n",
    "\n",
    "### There is one eigenspace for every eigenvalue. Each eigenspace is a vector space, so it can be described as the span of a basis. To compute the eigenspace of $\\lambda$, compute $\\operatorname{NS}(\\lambda\\bf{I}-\\bf{A})$ by Gaussian elimination and back-substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304007bc-ab6b-4ddf-87ae-c366d2756812",
   "metadata": {},
   "source": [
    "### ***Steps to find all eigenvectors associated to a given eigenvalue $\\lambda$ of a square matrix $\\bf{A}$***:\n",
    "### 1. Write down $\\lambda \\bf{I} - \\bf{A}$.\n",
    "### 2. Use Gaussian elimination to find a basis of $\\operatorname{NS}(\\lambda\\bf{I}-\\bf{A})$.\n",
    "### 3. The eigenvectors corresponding to the eigenvalue $\\lambda$ are all the linear combinations of these basis vectors (but not including the zero vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97fa4f-67ef-4a9a-a4e8-ede835cf8831",
   "metadata": {},
   "source": [
    "### ***Example 6.2***\n",
    "### Find all the eigenvalues, eigenvectors, and eigenspaces of $\\bf{A} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$.\n",
    "\n",
    "### ***Solution***:\n",
    "### The eigenvalues are $0$, $1$ and $2$. (We have found these before, by determining the roots of the $P(\\lambda) = \\det (\\lambda\\bf{I} - \\bf{A})$).\n",
    "### ***Eigenspace of*** $0$:\n",
    "### This is $\\operatorname{NS}(0\\bf{I}-\\bf{A}) = \\operatorname{NS}(-\\bf{A})=\\operatorname{NS}(\\bf{A})$, i.e. the set of all solutions to \n",
    "## $$ \\bf{A}\\bf{v} = \\bf{0} $$\n",
    "## $$ \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\bf{v} = \\bf{0} $$\n",
    "\n",
    "### The first and third row are the same, and so subtracting row $1$ from row $3$, we get\n",
    "## $$ \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\to \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}  $$\n",
    "### The resulting matrix is in reduced row echelon form. The null space of $\\bf{A}$ is:\n",
    "## $$ \\text{Eigenspace of}\\,0 = \\operatorname{NS}(\\bf{A}) = \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} $$\n",
    "### In other words, the eigenspace of the eigenvalue $0$ is the $1$-dimensional vector space spanned by the eigenvector $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, and the set of all eigenvectors is all nonzero scalar multiplies of $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Geometrically, the eigenspace consists of all vectors along the line containing $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, which is defined by $x = -z$, $y = 0$.\n",
    "### ***Terminology***:\n",
    "### Very often, we say \"the eigenvectors of $\\lambda$\" to mean a ***basis*** of the eigenspace of $\\lambda$ even though strictly speacking, the eigenvectors are all nonzero combinations of that basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681d3ae-e372-405c-8a30-445f8bfa4f34",
   "metadata": {},
   "source": [
    "### ***Eigenspace of*** $1$:\n",
    "### This is $\\operatorname{NS}(\\bf{I}-\\bf{A})$, i.e. the set of all solutions to\n",
    "## $$ (\\bf{I}-\\bf{A}) \\bf{v} = \\bf{0} $$\n",
    "## $$ \\begin{pmatrix} 1-1 & 0 & -1 \\\\ 0 & 1-1 & 0 \\\\ -1 & 0 & 1-1 \\end{pmatrix} \\bf{v} = \\begin{pmatrix} 0 & 0 & -1 \\\\ 0& 0 & 0 \\\\ -1 & 0 & 0 \\end{pmatrix}\\bf{v} = \\bf{0} $$\n",
    "### We can immediately see that the first and third components of $\\bf{v}$ must be zero, while the second component is a free parameter. Hence, the set of all solutions to $(\\bf{I}-\\bf{A})\\bf{v} = \\bf{0}$ is\n",
    "## $$ \\text{Eigenspace of}\\,1 = \\operatorname{NS}(\\bf{I}-\\bf{A}) = \\operatorname{Span}\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n",
    "\n",
    "### The eigenspace of the eigenvalue $1$ is the $1$-dimensional vector space spanned by the eigenvector $\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n",
    "### Geometrically, this consists of all vectors along the $y$-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3f171-5852-4821-8999-9f3d36138ba0",
   "metadata": {},
   "source": [
    "### ***Eigenspace of*** $2$:\n",
    "### This is $\\operatorname{NS}(2\\bf{I}-\\bf{A})$, i.e. the set of all solutions to\n",
    "## $$ (2\\bf{I}-\\bf{A}) \\bf{v} = \\bf{0} $$\n",
    "## $$ \\begin{pmatrix} 2-1 & 0 & -1 \\\\ 0 & 2-1 & 0 \\\\ -1 & 0 & 2-1 \\end{pmatrix} \\bf{v} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}\\bf{v} = \\bf{0} $$\n",
    "### This matrix can be reduced to:\n",
    "## $$ \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix} \\to \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "\n",
    "### This gives a set of all solutions to $(2\\bf{I}-\\bf{A})\\bf{v} = \\bf{0}$ to be\n",
    "## $$ \\text{Eigenspace of}\\,2 = \\operatorname{NS}(2\\bf{I}-\\bf{A}) = \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n",
    "\n",
    "### The eigenspace of the eigenvalue $2$ is the $1$-dimensional vector space spanned by the eigenvector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n",
    "### Geometrically, this is the line along the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ (the line defined by $x=z, y=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c75f9-a71a-49b7-9314-e9c5193aadf4",
   "metadata": {},
   "source": [
    "### ***Conclusion***:\n",
    "### The eigenvalues and corresponding eigenspaces of the matrix\n",
    "## $$ \\bf{A} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\n",
    "### are\n",
    "## $$ \\begin{array} {rcl}  \\text{eigenvalue} & & \\text{Corresponding eigenspace} \\\\ \\lambda = 0 & ; & \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\\\ \\lambda=1 & ; & \\operatorname{Span}\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\\\ \\lambda=2 & ; & \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{array}  $$\n",
    "### The eigenvectors for each eigenvalue are all nonzero vectors in the corresponding eigenspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac5e4f-3748-499f-84b8-b074cb9a593f",
   "metadata": {},
   "source": [
    "## Worked example\n",
    "\n",
    "### ***Problem 7.1***\n",
    "###  Find all the eigenvalues, eigenvectors, and eigenspaces of\n",
    "## $$ \\bf{A} = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix} $$\n",
    "\n",
    "### ***Solution***:\n",
    "### We found previously that the eignevalues are $0$, $-3$, where $-3$ is of multiplicity $2$.\n",
    "\n",
    "### ***Eigenspace of*** $0$:\n",
    "### This is $\\operatorname{NS}(\\bf{A})$, that is, the set of all solutions to\n",
    "## $$ \\bf{A}\\bf{v} = \\bf{0} $$\n",
    "## $$ \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix} \\bf{v} = \\bf{0} $$\n",
    "### We reduce the matrix to reduced row echelon form:\n",
    "## $$ \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix} \\to \\begin{pmatrix} -2 & 1 & 1 \\\\ 0 & -3/2 & 3/2 \\\\ 0 & 3/2 & -3/2 \\end{pmatrix} \\to \\begin{pmatrix} -2 & 1 & 1 \\\\ 0 & -3/2 & 3/2 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "## $$ \\to \\begin{pmatrix} -2 & 1 & 1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\to \\begin{pmatrix} -2 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\to \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### Hence, $z$ is a free parameter, and the eigenspace is given by\n",
    "## $$ \\text{Eigenspace of}\\, 0 = \\operatorname{NS}(\\bf{A}) = \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b73b5d-b673-4572-9a77-694554147e78",
   "metadata": {},
   "source": [
    "### ***Eigenspace of*** $-3$:\n",
    "### We use the fact that $\\operatorname{NS}(-3\\bf{I}-\\bf{A}) = \\operatorname{NS}(\\bf{A} + 3\\bf{I})$ to avoid negative sign errors. The set of all solutions to\n",
    "## $$ (\\bf{A}+3\\bf{I}) \\bf{v} = \\bf{0} $$\n",
    "## $$ \\begin{pmatrix} -2+3 & 1 & 1 \\\\ 1 & -2+3 & 1 \\\\ 1 & 1 & -2+3 \\end{pmatrix} \\bf{v} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} \\bf{v} = \\bf{0} $$\n",
    "### This matrix can be reduced to:\n",
    "## $$ \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} \\to \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### Hence, both $y$ and $z$ can be free parameters and $x=-y-z$. The eigenspace is given by\n",
    "## $$ \\text{Eigenspace of}\\, -3 = \\operatorname{NS}(\\bf{A}+3\\bf{I}) = \\operatorname{Span} \\left( \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right) $$\n",
    "### This is a $2$-dimensional vector space spanned by two independent vectors $\\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Geometrically, this consists of all vectors on the plane $x=-y-z$.\n",
    "\n",
    "### ***Conclusion***:\n",
    "### The eigenvalues and corresponding eigenspaces of the matrix $\\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$ are:\n",
    "## $$ \\begin{array} {rcl} \\text{Eigenvalue} & & \\text{Corresponding eigenspace} \\\\ \\lambda=0 & ; & \\operatorname{Span}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\\\ \\lambda=-3 & ; & \\operatorname{Span}\\left( \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right) \\end{array} $$\n",
    "### (The eigenvectors for each eigenvalue are all nonzero vectors in the corresponding eigenspace.) Notice that the eigenspace of $-3$ is $2$-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caba88-09ad-4a7e-8d7e-da982e393225",
   "metadata": {},
   "source": [
    "## Trace\n",
    "\n",
    "### Let us take a quick detour to discuss the relationship between eigenvalues and the trace and determinant of a matrix.\n",
    "\n",
    "### ***Definition 9.1***\n",
    "### The ***trace*** of a square matrix $\\bf{A}$ is the sum of the entries along the main diagonal. That is, for\n",
    "## $$ \\begin{pmatrix}  {\\color{orange}{a_{11}}} & \\cdots & \\cdots & a_{1n} \\\\ a_{21}& {\\color{orange}{a_{22}}} & \\cdots & a_{2n} \\\\ \\vdots & & \\ddots & \\vdots \\\\ a_{n1}& \\cdots & \\cdots & {\\color{orange}{a_{nn}}} \\end{pmatrix} $$\n",
    "### The trace is\n",
    "## $$ \\operatorname{tr} \\bf{A} = {\\color{orange}{a_{11}}} + {\\color{orange}{a_{22}}} + \\ldots + {\\color{orange}{a_{nn}}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252cd18-f4fc-4753-997e-86ae2ffdbc98",
   "metadata": {},
   "source": [
    "### ***Example 9.2***\n",
    "\n",
    "### If $ \\bf{A} = \\begin{pmatrix} {\\color{orange}{4}} & 6 & 9 \\\\ 1 & {\\color{orange}{7}} & 8 \\\\ 2 & 3 & {\\color{orange}{5}} \\end{pmatrix}$, then $\\operatorname{tr} \\bf{A} = {\\color{orange}{4}} + {\\color{orange}{7}} + {\\color{orange}{5}} = 16$.\n",
    "### ***Warning***\n",
    "### Recall that trace and determinant make sense only for ***square*** matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe3f30-6910-4729-9ec9-29884df03935",
   "metadata": {},
   "source": [
    "### ***Problem 9.3***\n",
    "### For an $n \\times n$ matrix $\\bf{A}$, how do $\\operatorname{tr}(-\\bf{A})$ and $\\det(-\\bf{A})$ relate to $\\operatorname{tr} \\bf{A}$ and $\\det \\bf{A}$?\n",
    "\n",
    "### ***Solution***\n",
    "### Negating $\\bf{A}$ negates in particular all diagonal entries of $\\bf{A}$, so $\\operatorname{tr}(-\\bf{A}) = -\\operatorname{tr} \\bf{A}$.\n",
    "### On the other hand, negating $\\bf{A}$ amounts to multiplying every row by $-1$, which multiplies $\\det \\bf{A}$ by $(-1)^n$ because there is one factor of $-1$ for each row. Thus\n",
    "### - If $n$ is even, then $\\det(-\\bf{A}) = \\det \\bf{A}$\n",
    "### - If $n$ is odd, then $\\det(-\\bf{A}) = -\\det \\bf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c8f35-1d68-421c-90d7-0e46f04b4222",
   "metadata": {},
   "source": [
    "## Eigenvalues, and trace and determinant\n",
    "\n",
    "### Recall that for any $2 \\times 2$ matrix $\\bf{A}$ the characteristic polynomial can also be written in terms of the trace and determinant as follows:\n",
    "## $$ \\lambda^2 - (\\operatorname{tr}\\bf{A})\\lambda + \\det\\bf{A} $$\n",
    "### ***Proof***\n",
    "### For a matrix $\\bf{A}=\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the characteristic polynomial is\n",
    "## $$ \\begin{array} {rcl} \\det (\\lambda\\bf{I}-\\bf{A}) & = & \\det \\begin{pmatrix} \\lambda-a & -b \\\\ -c & \\lambda-d \\end{pmatrix} \\\\ \\, & = & (\\lambda-a)(\\lambda-d)-bc \\\\ \\, & = & \\lambda^2-(a+d)\\lambda + (ad-bc) \\\\ \\, & = & \\lambda^2 -(\\operatorname{tr}\\bf{A})\\lambda+(\\det\\bf{A}) \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235e66b-ff23-4616-a9c4-bad92368966d",
   "metadata": {},
   "source": [
    "### For an $n \\times n$ matrix $\\bf{A}$, where $n \\geq 2$, it turns out that the characteristic polynomial, multiplied by $\\pm 1$ to make the leading coefficient $1$, takes the form\n",
    "## $$ \\lambda^n - (\\operatorname{tr}\\bf{A})\\lambda^{n-1} + \\ldots \\pm \\det\\bf{A} $$\n",
    "### where the $\\pm$ is $+$ if $n$ is even, and $-$ if $n$ is odd.\n",
    "### So knowing $\\operatorname{tr}\\bf{A}$ and $\\det \\bf{A}$ determines $2$ coefficients of the characteristic polynomial. More importantly, since\n",
    "## $$ \\lambda^n - (\\operatorname{tr}\\bf{A})\\lambda^{n-1} + \\ldots \\pm \\det \\bf{A} = (\\lambda-\\lambda_1)(\\lambda-\\lambda_2)\\cdots(\\lambda-\\lambda_n) $$\n",
    "### where $\\lambda_1,\\ldots,\\lambda_n$ are the $n$ (not necessarily distinct) eigenvalues. Comparing coefficients gives\n",
    "## $$ \\operatorname{tr}(\\bf{A}) = \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n $$\n",
    "## $$ \\det(\\bf{A}) = (\\lambda_1)(\\lambda_2)\\cdots(\\lambda_n) $$\n",
    "### In other words, the trace is the ***sum*** of all $n$ (not necessarily distinct) eigenvalues; the determinant is the ***product*** of all $n$ eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d924131a-0013-42d3-b200-9a3a927db72f",
   "metadata": {},
   "source": [
    "## Complex eigenvalues and eigenvectors\n",
    "\n",
    "### Recall that complex non-real eigenvalues come in pairs of complex conjugates. It turns out the eigenvectors of a complex eigenvalue are the complex conjugates of the eigenvectors of the conjugate eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880579b2-1327-4c17-99e2-0c168c6cd74a",
   "metadata": {},
   "source": [
    "### ***Example 11.1***\n",
    "### Find the eigenvalues and eigenvectors of the matrix $\\bf{A} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\n",
    "### We have found previously that the eigenvalues are $1$, $\\pm i$. Let us now find the eigenvectors. We will start with those of the complex eigenvalue $i$.\n",
    "\n",
    "### ***Eigenspace of*** $i$: $\\operatorname{NS}(i\\bf{I}-\\bf{A})$\n",
    "### We reduce $i\\bf{I}-\\bf{A}$ to row echelon form\n",
    "## $$ \\begin{array} {rcccl} i\\bf{I}-\\bf{A} & = & \\begin{pmatrix} i & 1 & 0 \\\\ -1 & i & 0 \\\\ 0 & 0 & i-1 \\end{pmatrix} & \\to & \\begin{pmatrix} i & 1 & 0 \\\\ -1-(i*i) & i-(i*1) & 0-(i*0) \\\\ 0 & 0 & i-1 \\end{pmatrix} \\\\ \\, & \\to & \\begin{pmatrix} i & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & i-1 \\end{pmatrix} & \\to & \\begin{pmatrix} i & 1 & 0 \\\\ 0 & 0 & i-1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\\\ \\, & \\to & \\begin{pmatrix} i & 1 & 0 \\\\ \\frac{0}{1-i} & \\frac{0}{1-i} & \\frac{i-1}{1-i} \\\\ 0 & 0 & 0 \\end{pmatrix} & \\to & \\begin{pmatrix} i & 1 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\end{array} $$\n",
    "\n",
    "### By back substitution (the first equation is $i x + y = 0$), the solutions are $c \\begin{pmatrix} i \\\\ 1 \\\\ 0 \\end{pmatrix}$ for any scalar $c$. That is,\n",
    "## $$ \\text{Eigenspace of}\\, i = \\operatorname{NS}(i \\bf{I} - \\bf{A}) = \\operatorname{Span}\\begin{pmatrix} i \\\\ 1 \\\\ 0 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5fb85-4161-4c9c-8086-40438072ab5c",
   "metadata": {},
   "source": [
    "### ***Eigenspace of*** $-i$: $\\operatorname{NS}(-i\\bf{I}-\\bf{A})$\n",
    "### Since $\\bf{A}$ is a real matrix, $-i\\bf{I}-\\bf{A}$ is the complex conjugate of $i\\bf{I}-\\bf{A}$, and every step in the Gaussian elimination is the same as before with $i$ replaced by $-i$:\n",
    "## $$ \\begin{array} {rcccl} -i\\bf{I}-\\bf{A} & = & \\begin{pmatrix} -i & 1 & 0 \\\\ -1 & -i & 0 \\\\ 0 & 0 & -i-1 \\end{pmatrix} & \\to & \\begin{pmatrix} -i & 1 & 0 \\\\ -1+(i*(-i)) & -i+(i*1) & 0+(i*0) \\\\ 0 & 0 & i-1 \\end{pmatrix} \\\\ \\, & \\to & \\begin{pmatrix} -i & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -i-1 \\end{pmatrix} & \\to & \\begin{pmatrix} i & -1 & 0 \\\\ 0 & 0 & -i-1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\\\ \\, & \\to & \\begin{pmatrix} i & -1 & 0 \\\\ \\frac{0}{1+i} & \\frac{0}{1+i} & \\frac{-i-1}{1+i} \\\\ 0 & 0 & 0 \\end{pmatrix} & \\to & \\begin{pmatrix} i & -1 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\end{array} $$\n",
    "\n",
    "### Therefore, the row echelon form of $-i\\bf{I}-\\bf{A}$ s the complex conjugate of the row echelon form of $i\\bf{I}-\\bf{A}$ and the vectors in the nullspace of $-i\\bf{I}-\\bf{A}$ are the complex conjugates of the vectors in the nullspace of $i\\bf{I}-\\bf{A}$. Hence\n",
    "## $$ \\text{Eigenspace of}\\,-i=\\operatorname{NS}(-i\\bf{I}-\\bf{A})=\\operatorname{Span}\\begin{pmatrix} -i \\\\ 1 \\\\ 0 \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755c6a9-4328-412c-88ab-012bb9ebcc2a",
   "metadata": {},
   "source": [
    "### ***Eigenspace of*** $1$: $\\operatorname{NS}(\\bf{I}-\\bf{A})$\n",
    "### We start by reducing $\\bf{I}-\\bf{A}$:\n",
    "## $$ \\bf{I}-\\bf{A} = \\begin{pmatrix} 1 & 1 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\to \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### This gives the eigenspace\n",
    "## $$ \\text{Eigenspace of}\\,1=\\operatorname{NS}(\\bf{I}-\\bf{A})=\\operatorname{Span}\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n",
    "### (You could also have guessed the solution $\\bf{v} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$ and plugged it into $\\bf{A}\\bf{v}$ to check that indeed it is an eigenvector of eigenvalue $1$.)\n",
    "### ***Conclusion***:\n",
    "### The eigenvalues and corresponding eigenspaces of matrix $\\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ are:\n",
    "## $$ \\begin{array} {rcl} \\text{Eigenvalue} & \\, & \\text{Corresponding eigenspace} \\\\ \\lambda=i & ; & \\operatorname{Span} \\begin{pmatrix} i \\\\ 1 \\\\ 0 \\end{pmatrix}  \\\\ \\lambda=-i & ; & \\operatorname{Span} \\begin{pmatrix} -i \\\\ 1 \\\\ 0 \\end{pmatrix} \\\\ \\lambda=1 & ; & \\operatorname{Span}\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{array}$$\n",
    "### (The eigenvectors for each eigenvalue are all nonzero vectors in the corresponding eigenspace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc70973-f162-4c04-a5b3-89d5fb15f90c",
   "metadata": {},
   "source": [
    "## Dimension of an eigenspace\n",
    "\n",
    "### ***Theorem 12.1***\n",
    "### Let $\\lambda$ be an eigenvalue of an $n \\times n$ matrix $\\bf{A}$. Suppose that the multiplicity of $\\lambda$ (as a root of the characteristic polynomial) is $m$. Then\n",
    "## $$ 1 \\leq (\\text{dimension of eigenspace of}\\, \\lambda) \\leq m $$\n",
    "### Given an eigenvalue $\\lambda$, the dimension of the eigenspace of $\\lambda$ is the maximum number of linearly independent eigenvectors corresponding to $\\lambda$. This dimension is at least $1$ since there is at least one eigenvector of eigenvalue $\\lambda$ (otherwise $\\lambda$ would not have been an eigenvalue). That this dimension is at most $m$ requires more work to prove and we will not prove this in this course. One of the main ingredients is that eigenvectors corresponding to different eigenvalues must be linearly independent, which we will show below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca7c3b-8d72-4a21-8e86-c4b7c3ca9642",
   "metadata": {},
   "source": [
    "### ***Example 12.2***\n",
    "### A $9 \\times 9$ matrix has the characteristic polynomial $(\\lambda-2)^3(\\lambda-5)^6$. What are the possibilities for the dimension of the eigenspace of $2$?\n",
    "\n",
    "### ***Solution***\n",
    "### In this case, $m=3$, so the dimension is $1$, $2$, or $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e9d0a-d3a5-4f6c-a18f-bd3e5df8c76e",
   "metadata": {},
   "source": [
    "### ***Definition 12.3***\n",
    "### - The eigenvalue $\\lambda$ and its eigenspace are called ***complete*** (or ***perfect***) if the dimension of the eigenspace equals the multiplicity $m$ of $\\lambda$.\n",
    "### - The eigenvalue $\\lambda$ and its eigenspace are called ***deficient*** (***defective***, ***degenerate***, or ***incomplete***) if the dimension of the eigenspace is less than $m$.\n",
    "### - A ***matrix*** is ***complete*** if ***all*** of ot's eigenspaces are complete; its ***deficient*** if any ***one*** of its eigenspaces is deficient.\n",
    "\n",
    "### ***Warning***\n",
    "### Different authors use different terminology here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a2018-3144-469b-830f-cad104da62f5",
   "metadata": {},
   "source": [
    "### ***Example 12.4***\n",
    "### If the multiplicity of an eigenvalue is $1$, then the dimension of its eigenspace is sandwitched between $1$ and $1$, and so must be $1$. In other words, the eigenspace of any eigenvalue of multiplicity $1$ is complete.\n",
    "\n",
    "### ***Theorem 12.5***\n",
    "### Fix an $n \\times n$ matrix $\\bf{A}$. The eigenvectors corresponding to distinct eigenvalues are linearly independent.\n",
    "\n",
    "### ***Proof***\n",
    "### Suppose that $\\bf{v_1}, \\ldots, \\bf{v_n}$ are eigenvectors corresponding to distinct eigenvalues $\\lambda_1, \\ldots, \\lambda_n$. Suppose that there were a linear relation\n",
    "## $$ c_1\\bf{v_1}+\\cdots+c_p\\bf{v_p} $$\n",
    " \n",
    "### Apply $\\lambda_1\\bf{I} - \\bf{A}$ to both sides; this gets rid of the first summand on the left. Next apply $\\lambda_2\\bf{I}-\\bf{A}$, and so on, up to $\\lambda_{p-1}\\bf{I}-\\bf{A}$. This shows that some nonzero number times $c_p\\bf{v_p}$ equals $\\bf{0}$. But $\\bf{v_p} \\neq \\bf{0}$, so $c_p = 0$. Similarly each $c_i$ must be $0$. Thus only the trivial relation between $\\bf{v_1},\\ldots,\\bf{v_n}$ exists, so they are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0219022-6510-44a6-82b0-3912b61a9e27",
   "metadata": {},
   "source": [
    "### As a consequence,\n",
    "### ***Fact***: If all the of a $n \\times n$ matrix $\\bf{A}$ are complete, then there is a set of $n$ linearly independent eigenvectors of $\\bf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc5bd-227c-4bbd-8332-05cd660f0967",
   "metadata": {},
   "source": [
    "### ***Proof***\n",
    "### Choose a basis for each eigenspace and concatenate these lists of eigenvectors. Since all the eigenspaces are complete, the number of linearly independent eigenvectors from each eigenspace is the multiplicity of the eigenvalue. So the total number of eigenvectors in our list is the sum of the multiplicities of the eigenvalues, which is , the degree of the characteristic polynomial.\n",
    "### The theorem above tells us that eigenvectors of different eigenvalues are linearly independent, so our list consists of all linearly independent eigenvectors. (This list forms a basis of $\\mathbb{C}^n$ or $\\mathbb{R}^n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e723382-fcc2-4687-89d5-0eff3d875ce0",
   "metadata": {},
   "source": [
    "### ***Fact***:\n",
    "### If instead the matrix $\\bf{A}$ is deficient, then the maximum number of linearly independently eigenvectors is less than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d1461-f2e7-46fd-9a2e-2e1ea65b78a9",
   "metadata": {},
   "source": [
    "### ***Example 12.6***\n",
    "### Recall for the matrix $\\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$, the eigenvalues and corresponding eigenvectors are:\n",
    "## $$ \\begin{array} {rcl} \\text{Eigenvallue} & \\, & \\text{Eigenvectors (basis of eigenspace)} \\\\ \\lambda=0 & ; & \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\\\ \\lambda=1 & ; & \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\\\ \\lambda=2 & ; & \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{array} $$\n",
    "### There are $3$ distinct eigenvalues (each of multiplicity $1$). So the corresponding eigenvectors $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$, and $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ are linearly independent and form a basis of $\\mathbb{R}^3$ (or $\\mathbb{C}^3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522dc88-c55e-428e-bc9e-a92be0d8b567",
   "metadata": {},
   "source": [
    "### ***Example 12.7***\n",
    "### For $\\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$, the eigenvalues and corresponding eigenvectors are\n",
    "## $$ \\begin{array} {rcl} \\text{Eigenvalue} & \\, & \\text{Eigenvectors (basis of eigenspace)} \\\\ \\lambda = 0 & ; & \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\\\ \\lambda=-3 & ; & \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{array} $$\n",
    "### The eigenvalue $-3$ has multiplicity $2$ (as a root of the characteristic polynomial) and its eigenspace is of dimension $2$ and has a basis $\\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. These two eigenvectors of $-3$ together with any one eigenvector of the eigenvalue $0$ form a basis of $\\mathbb{R}^3$ since the $3$ vectors must be linearly independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58bebc-92b0-4f82-b625-e815168bd9be",
   "metadata": {},
   "source": [
    "## A deficient matrix\n",
    "### In this course, we will only deal with linear systems of DEs described by complete (not deficient) matrices. Nonetheless, here is an example of a deficient matrix.\n",
    "\n",
    "### ***Example 12.8***   \n",
    "### The matrix $\\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$ has characteristic polynomial is $(\\lambda - 1)^2$, and so its only eigenvalue is $1$ with multiplicity $2$. However, the eigenspace of $1$ is $\\operatorname{Span} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ which is only $1$-dimensional. Hence, this matrix is ***deficient***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a5b09-56d7-4bd9-8f8a-4e8f2f36a90b",
   "metadata": {},
   "source": [
    "## Symmetric matrices\n",
    "\n",
    "### The transpose of a matrix $\\bf{A}$ is defined to be the matrix $\\bf{A}^T$ formed by turning the rows of $\\bf{A}$ into the columns of $\\bf{A}^T$. In other words, the entry in the $i$th row and $j$th column of $\\bf{A}^T$ is the entry in the $j$th row and $i$th column of $\\bf{A}$:\n",
    "## $$ (\\bf{A}^T)_{ij} = \\bf{A}_{ji} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8a432-289c-40a1-9af8-4b5f2d122668",
   "metadata": {},
   "source": [
    "### ***Example 12.9***\n",
    "### The transpose of $\\bf{A} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$ is the matrix\n",
    "## $$ \\bf{A}^T = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n",
    "### A ***symmetric matrix*** is a square $n \\times n$ matrix $\\bf{A}$ that is equal to its transpose.\n",
    "## $$ \\bf{A}^T = \\bf{A} $$\n",
    "### In other words, the entry in the $i$th row and $j$th column is equal to the entry in the $j$th row and $i$th column\n",
    "## $$ a_{ij} = a_{ji} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2df9e1-2f17-43f3-a84b-3c442cbecc32",
   "metadata": {},
   "source": [
    "### ***Example 12.10***\n",
    "### The matrix $\\bf{A} = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$ is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09778d96-22d9-4808-9c9b-c543f7270221",
   "metadata": {},
   "source": [
    "## Spectral Theorem:\n",
    "### If $\\bf{A}$ is a real, symmetric, $n \\times n$ matrix, then all of its eigenvalues are complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411e8b2-6df3-4e97-818e-b7dfbc774992",
   "metadata": {},
   "source": [
    "## Diagonalization\n",
    "\n",
    "### Suppose that $\\bf{A}$ is a $2 \\times 2$ matrix with a basis of eigenvectors $\\bf{v_1}$, $\\bf{v_2}$ corresponding to the distinct eigenvalues $\\lambda_1$, $\\lambda_2$ respectively (i.e. $\\bf{A}$ is complete).\n",
    "\n",
    "### ***Warning***: \n",
    "### For what we are about to do, the eigenvectors must be listed in the same order as their eigenvalues.\n",
    "### Use the eigenvalues to define a diagonal matrix $\\bf{D}$:\n",
    "## $$ \\bf{D}= \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix} $$\n",
    "### The matrix mapping $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ to $\\bf{v_1}$, $\\bf{v_2}$, respectively, is the matrix\n",
    "## $$ \\bf{S}= \\begin{pmatrix} | & | \\\\ \\bf{v_1} & \\bf{v_2} \\\\ | & | \\\\ \\end{pmatrix} $$\n",
    "### whose columns are the eigenvectors $\\bf{v_1}$, $\\bf{v_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f5f44-00e2-4476-bc65-a7e070d16645",
   "metadata": {},
   "source": [
    "### ***Question 13.1***\n",
    "### What is the $2 \\times 2$ matrix that maps $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ to $\\lambda_1 \\bf{v_1}$, $\\lambda_2 \\bf{v_2}$, respectively?\n",
    "\n",
    "### The picture below illustrates two ways of getting $\\lambda_1 \\bf{v_1}$, $\\lambda_2 \\bf{v_2}$ out of $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ respectively. Based on that image, construct two matrices that answer the question.\n",
    "![IMG](img/linear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccc45e-68f3-4545-9f25-f34ec913341e",
   "metadata": {},
   "source": [
    "### ***Answer 1***\n",
    "### $\\bf{A}\\bf{S}$, because applying $\\bf{A}\\bf{S}$ means that $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ are mapped by $\\bf{S}$ to $\\bf{v_1}$, $\\bf{v_2}$, which are then mapped by $\\bf{A}$ to $\\lambda_1\\bf{v_1}$, $\\lambda_2 \\bf{v_2}$.\n",
    "\n",
    "### ***Answer 2***\n",
    "### $\\bf{S}\\bf{D}$, because $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ are mapped by $\\bf{D}$ to $\\begin{pmatrix} \\lambda_1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ \\lambda_2 \\end{pmatrix}$ which are then mapped by $\\bf{S}$ to $\\lambda_1\\bf{v_1}$, $\\lambda_2 \\bf{v_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d0cb3-038e-415b-8146-391be91700e6",
   "metadata": {},
   "source": [
    "### ***Conclusion***:\n",
    "### $\\bf{A}\\bf{S} = \\bf{S}\\bf{D}$\n",
    "### (Memory aid: Just as in eigenvalue-eigenvector equation $\\bf{A}\\bf{v} = \\lambda\\bf{v}$ where $\\bf{A}$ is applied to an eigenvector $\\bf{v}$, here $\\bf{A}$ should be applied to the matrix whose columns are the eigenvectors, hence $\\bf{S}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e13201-719c-483e-99ac-7c46b975cab0",
   "metadata": {},
   "source": [
    "### Multiply by $\\bf{S}^{-1}$ on the right to get another way to write it:\n",
    "## $$ \\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1} \\quad \\text{where}\\, \\bf{S} = \\begin{pmatrix} | & | \\\\ \\bf{v_1} & \\bf{v_2} \\\\ | & | \\end{pmatrix} $$\n",
    "\n",
    "### Writing the matrix $\\bf{A}$ like this is clled ***diagonalizing*** $\\bf{A}$.\n",
    "### ***Remark***:\n",
    "### We can think of the matrix $\\bf{S} = (\\bf{v_1}, \\bf{v_2})$ as a \"coordinate-change matrix\" or \"change-of-basis matrix\" that\n",
    "### - relates the standard basis $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ to the basis of eigenvectors of $\\bf{A}$, and\n",
    "### - relates the easy matrix $\\bf{D}$ scaling the standard basis vectors to the original matrix $\\bf{A}$ scaling the original eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25697250-5eb0-45ac-b031-dc44be4edbaa",
   "metadata": {},
   "source": [
    "## Symmetric matrices and their diagonalization\n",
    "\n",
    "### Note that because the eigenvalues of a real symmetric matrix are always complete, a real symmetric matrix is always diagonalizable. More, it turns out, is true for symmetric matrices.\n",
    "### ***Fun Fact 1*** If $\\bf{A}$ is a symmetric, real $n \\times n$ matrix, its eigenvalues are real.\n",
    "### Two vectors $\\bf{v}$ and $\\bf{w}$ are ***orthogonal*** if their dot product is zero. The dot product of the two vectors $\\bf{v}$ and $\\bf{w}$ can be written as $\\bf{v}^T\\bf{w}$ using ordinary matrix multiplication.\n",
    "### ***Fun Fact 2*** If $\\bf{A}$ is a symmetric, real $n \\times n$ matrix, the eigenvectors of any two distinct eigenvalues are orthogonal.\n",
    "### In particular, this means that we can find a basis for the eigenspaces that are all mutually orthogonal. This is because any repeated eigenvalue has a complete eigenspace, and there is an algorithm, called the Gram–Schmidt algorithm, that allows us to find an orthogonal set of vectors.\n",
    "### Why do we care about orthogonal vectors?\n",
    "### Let $\\bf{v_1}, \\ldots,\\bf{v_n}$ be a collection of eigenvectors of $\\bf{A}$ that are pairwise orthogonal, that is\n",
    "## $$ \\bf{v_i}^T\\bf{v_j} = 0 \\quad \\text{if}\\,i\\neq j $$\n",
    "### Normalize these vectors so that each of them has length one, that is\n",
    "## $$ \\bf{v_i}^T\\bf{v_i} = 1 $$\n",
    "### Once the vectors are normalized in this manner, they are said to be ***orthonormal***.\n",
    "\n",
    "### Let $\\bf{S}$ be the matrix whose columns are these orthonormal vectors. Then we say that $\\bf{S}$ is an ***orthogonal matrix*** , and has the special property that $\\bf{S}^{-1} = \\bf{S}^T$. This is convenient because computing inverses is often computationally intensive, but the transpose is simple.\n",
    "### In particular, the diagonalization of the matrix $\\bf{A}$ can be expressed as $\\bf{A} = \\bf{S}\\bf{D}\\bf{S}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2bbfd-a346-49e0-b7d7-368ca5ce2307",
   "metadata": {},
   "source": [
    "## Examples of diagonalization\n",
    "\n",
    "### ***Steps to diagonalize an $n \\times n$ matrix $\\bf{A}$***:\n",
    "### ***Step 1***. Find the eigenvalues and eigenvectors of $\\bf{A}$.\n",
    "### ***Step 2***. Check that there are enough linearly independent eigenvectors $\\bf{v_1}, \\bf{v_2}, \\ldots,\\bf{v_n}$ to form a basis of $\\mathbb{R}^n$ (or $\\mathbb{C}^n$).\n",
    "### If yes, set\n",
    "## $$ \\bf{D} = \\begin{pmatrix} \\lambda_1 & \\, & \\, & \\, \\\\ \\, & \\lambda_2 & \\, & \\, \\\\ \\, & \\, & \\ddots & \\, \\\\ \\, & \\, & \\, & \\lambda_n \\end{pmatrix} $$\n",
    "## $$ \\bf{S} = \\begin{pmatrix} | & | & \\cdots & | \\\\ \\bf{v_1} & \\bf{v_2} & \\cdots & \\bf{v_n} \\\\ | & | & \\cdots & | \\end{pmatrix} \\quad \\text{where }\\, \\bf{v_i}\\,\\text{corresponds to}\\,\\lambda_i $$\n",
    "### ***Important***:\n",
    "### The eigenvectors must be listed in the same order as their eigenvalues.\n",
    "### If any of the eigenspaces is deficient, there will not be enough linearly independent eigenvectors, and $\\bf{A}$ is ***not***  diagonalizable.\n",
    "### ***Step 3***. Write $\\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dd45a-245a-4b18-bf02-52999625c5e7",
   "metadata": {},
   "source": [
    "### ***Example 15.1***\n",
    "### Let us diagonalize the matrix $\\bf{A} = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}$\n",
    "### ***Step 1***. The eigenvalues and eigenvectors of $\\bf{A}$:\n",
    "## $$ \\begin{array} {rcl}  \\text{Eigenvalue} & & \\text{Eigenvectors (basis of eigenspace)} \\\\ \\lambda = 0 & ; & \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\\\ \\lambda = -2 & ; & \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\end{array} $$\n",
    "### ***Step 2***. The eigenvalues are of multiplicity $1$, so the matrix is complete. Indeed, the two eigenvectors $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ form a basis of $\\mathbb{R}^2$. Set\n",
    "## $$ \\bf{D} = \\begin{pmatrix} 0 & 0 \\\\ 0 & -2 \\end{pmatrix} $$\n",
    "## $$ \\bf{S} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} $$\n",
    "### where the first column of $\\bf{S}$ is the eigenvector of the eigenvalue $0$, listed in the first diagonal entry of $\\bf{D}$ and the second column of $\\bf{S}$ is the eigenvector of $-2$ listed in the second diagonal entry in $\\bf{D}$.\n",
    "### ***Step 3***. Write $\\bf{A}=\\bf{S}\\bf{D}\\bf{S}^{-1}$. If desired, compute $\\bf{S}^{-1} = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix}$ to get the explicit formula:\n",
    "## $$ \\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1} $$\n",
    "## $$ \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7d140-e6bf-48dd-9e84-72e79ceeffc7",
   "metadata": {},
   "source": [
    "### ***Problem 15.2***\n",
    "### Diagonalize $\\bf{A} = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$.\n",
    "### ***Solution***\n",
    "### We have previously found the eigenvalues and eigenvectors of $\\bf{A}$:\n",
    "## $$ \\begin{array} {rcl}  \\text{Eigenvalue} & & \\text{Eigenvectors (basis of eigenspace)} \\\\ \\lambda = 0 & ; & \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\\\ \\lambda = -3 & ; & \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\end{array} $$\n",
    "### Since each eigenspace is complete and there are $3$ linearly independent eigenvectors, $\\bf{A}$ is diagonalizable:\n",
    "## $$ \\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1} \\quad \\text{where} \\quad \\bf{D}=\\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & -3 & 0 \\\\ 0 & 0 & -3 \\end{pmatrix}, \\bf{S} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c0b5e-3e47-4772-84bb-914c8843b382",
   "metadata": {},
   "source": [
    "## Applications of diagonalization\n",
    "\n",
    "### Suppose that $\\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1}$. Then\n",
    "## $$ \\bf{A}^3 = \\bf{S}\\bf{D}\\underbrace{\\bf{S}^{-1} \\bf{S}}_{{\\color{red}{\\textrm{cancels}}} } \\bf{D}\\underbrace{\\bf{S}^{-1} \\bf{S}}_{{\\color{red}{\\text{cancels}}} } \\bf{D}\\bf{S}^{-1} = \\bf{S}\\bf{D}^3 \\bf{S}^{-1} $$\n",
    "### More generally, for any integer $n \\geq 0$,\n",
    "## $$ \\bf{A}^n = \\bf{S}\\bf{D}^n\\bf{S}^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84b1e7-e71c-4474-a3a0-2e5ef3497636",
   "metadata": {},
   "source": [
    "### ***Problem 16.1***\n",
    "### Compute $\\bf{A}^{10}$ for $\\bf{A} = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}$\n",
    "### ***Solution***\n",
    "### We have previously diagonalized this matrix:\n",
    "## $$ \\bf{A} = \\bf{S}\\bf{D}\\bf{S}^{-1} \\quad \\text{where}\\quad \\bf{S} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}, \\bf{D}=\\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & -3 & 0 \\\\ 0 & 0 & -3 \\end{pmatrix}$$\n",
    "### This gives\n",
    "## $$ \\begin{array} {rcl} \\bf{A}^{10} & = & (\\bf{S}\\bf{D}\\bf{S}^{-1})^{10} \\\\ \\, & = & \\bf{S}\\bf{D}^{10}\\bf{S}^{-1} \\\\ \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & (-3)^{10} & 0 \\\\ 0 & 0 & (-3)^{10} \\end{pmatrix} \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}^{-1}  \\end{array} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c59ae8-22e8-4913-afdd-30a24fb7c1f0",
   "metadata": {},
   "source": [
    "## Application: the Pagerank Algorithm\n",
    "### While we've studied a lot of physical systems in this class, there are many non-physical systems that use linear algebra. One rich source of applications is computational science. We'll give one example here, an early version of the ***pagerank algorithm*** used by Google and other search engines to determine the order of results in a query. To define this rigorously, we'll use the tools of linear algebra. What we will do here will not be the exact pagerank algorithm, but will get across the general idea while also providing some linear algebra practice.\n",
    "\n",
    "### ***Defnitions and Problems***\n",
    "### In order to show how pagerank works, we'll work through a series of examples. First imagine we have three webpages: Wikipedia, Facebook, and Buzzfeed. Let's assume this is the entire internet, and the percentage of people on each page at a time is $W$, $F$, and $B$ respectively. Now we need to find the number of links between each page. We'll store this information in a matrix, calling this matrix . Starting with no links, we write\n",
    "## $$ \\bf{P} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "\n",
    "### Now to start filling in this matrix. Suppose Buzzfeed has six links to Facebook, and four links to Wikipedia. In our matrix, the column represents where the link is coming ***from***, and the row represents where the link is going ***to***. So for the links from Buzzfeed to Facebook, the matrix becomes\n",
    "## $$ \\bf{P} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### and then dding the links to Wikipedia, we get\n",
    "## $$ \\bf{P} = \\begin{pmatrix} 0 & 0 & 4 \\\\ 0 & 0 & 6 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### The matrix we are constructing is called the adjacency matrix. Each entry represents a connection from one node to another in a graph. This is a useful way of storing information dealing with different objects and directed connections between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda59a0-f813-495a-9808-8e5f008604ea",
   "metadata": {},
   "source": [
    "### ***Problem 17.1***   \n",
    "### Fill in the remainder of the adjacency matrix. Suppose that Wikipedia has one link to each Facebook and Buzzfeed, and Facebook has one link to Buzzfeed and two links to Wikipedia. What is our resultant matrix?\n",
    "### ***Solution***\n",
    "## $$ \\bf{P} = \\begin{pmatrix} 0 & 2 & 4 \\\\ 1 & 0 & 6 \\\\ 1 & 1 & 0 \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ca3a3-3057-4ae7-8aa0-a156dbbacaec",
   "metadata": {},
   "source": [
    "### Now that we have an adjacency matrix constructed, we can move forward and begin to attempt a ranking. To do this, we have to find the eigenvalues.\n",
    "### ***Problem 17.2***\n",
    "### Find the eigenvalues of the matrix $\\bf{P}$. Which eigenvalue has the largest absolute value?\n",
    "### ***Solution***\n",
    "### The eigenvalues are the solutions of the equation\n",
    "## $$ 0 = \\det(\\bf{R}-\\lambda\\bf{I}) $$ \n",
    "### Plugging in numbers we get\n",
    "## $$ 0 = \\det \\begin{pmatrix} -\\lambda & 2 & 4 \\\\ 1 & -\\lambda & 6 \\\\ 1 & 1 & -\\lambda \\end{pmatrix} $$\n",
    "### Computing the determinant gives\n",
    "## $$ 0 = -\\lambda^3+4+12+6\\lambda+2\\lambda+4\\lambda $$\n",
    "### Simplifying we see that\n",
    "# $$ 0 = \\lambda^3-12\\lambda-16 $$\n",
    "### We search for a root and find one, $\\lambda_1=-2$, and then factoring gives\n",
    "## $$0=(\\lambda+2)(\\lambda^2-2\\lambda-8) $$\n",
    "### Now factoring the last term we get\n",
    "## $$ 0=(\\lambda+2)(\\lambda+2)(\\lambda-4) $$\n",
    "### so our last two eigenvalues are\n",
    "## $$ \\lambda_2\t=-2, \\lambda_3=4 $$\n",
    "### Now we have found three eigenvalues, and the eigenvalue with the largest absolute value is $4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ea0f8-ac74-40f4-8fd8-2b290ed28578",
   "metadata": {},
   "source": [
    "### Why do we care about the eigenvalue that has the largest absolute value? Qualitatively, if we watch the path of a user as they browse the web, this is like applying the matrix to an initial state vector repeatedly. The eigenvector with the largest eigenvalue will dominate. So this eigenvector will give us the best ranking.\n",
    "### ***Problem 17.3***   \n",
    "### Find the eigenvector corresponding to the eigenvalue of largest absolute value.\n",
    "\n",
    "### ***Solution***\n",
    "### Finding the eigenvector is equivalent to finding the null space of the matrix\n",
    "## $$ 4\\bf{I}-\\bf{P} = \\begin{pmatrix}4 & -2 & -4 \\\\ -1 & 4 & -6 \\\\ -1 & -1 & 4  \\end{pmatrix} $$\n",
    "### To find the null space we use Gauss–Jordan elimination. We divide the first row by 4 and then add it to the second and third rows getting\n",
    "## $$ \\begin{pmatrix} 1 & -\\frac{1}{2} & -1 \\\\ 0 & \\frac{7}{2} & -7 \\\\ 0 & \\frac{3}{2} & -3 \\end{pmatrix} $$\n",
    "### Now we divide the second row by 7/2 to get\n",
    "## $$ \\begin{pmatrix} 1 & \\frac{1}{2} & -1 \\\\ 0 & 1 & -2 \\\\ 0 & \\frac{3}{2} & -3 \\end{pmatrix} $$\n",
    "### We now subtract $\\frac{3}{2}(\\text{row} 2)$ from the $\\text{row} 3$, and add $\\frac{1}{2}(\\text{row} 2)$ to the $\\text{row} 1$, to get\n",
    "## $$ \\begin{pmatrix} 1 & 0 & -2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### Now from inspection we see that a vector in the nullspace is\n",
    "## $$ \\bf{v} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} $$\n",
    "### Any vector in the nullspace is an eigenvector, so $\\bf{v}$ is an eigenvector of $\\bf{P}$. Now we can use this vector as a rough ranking of our websites; Wikipedia and Facebook each have a rank of 2, and Buzzfeed has a rank of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d6247-d088-4298-a0a4-10d16e580a24",
   "metadata": {},
   "source": [
    "### The algorithm we just described gives us a simple way of ranking the pages on the internet, but it has various issues that could be improved. Note that we didn't consider self-links in our count, which is why the diagonal of our adjacency matrix was zero. But what if an unscrupulous programmer from Wikipedia heads over to Buzzfeed and posts Wikipedia links everywhere? It's not hard to check and see that this causes the rank of Wikipedia to rise drastically. We want a system that is robust against a single site copying the same link over and over. A simple way to do this is to normalize the values in each column to add to one. This gets rid of the ability of any one website to largely skew the ranking. Also, this has probabilistic implications, but those are beyond the level of this class (for more information look into Markov Processes).\n",
    "\n",
    "### If we do this we are left with the following matrix\n",
    "## $$ \\begin{pmatrix} 0 & \\frac{2}{3} & \\frac{2}{5} \\\\ \\frac{1}{2} & 0 & \\frac{3}{5} \\\\ \\frac{1}{2} & \\frac{1}{3} & 0  \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e15b55-5459-4d7d-bd20-d1886ad4ea64",
   "metadata": {},
   "source": [
    "### ***Problem 18.1***\n",
    "### Find the largest absolute-value eigenvalue and corresponding eigenvector of the above matrix.\n",
    "### ***Solution***\n",
    "### We follow the steps we went through previously, first finding the eigenvalues by using\n",
    "## $$ 0 = \\det(\\lambda\\bf{I} - \\bf{M}) $$\n",
    "### And when we plug in numbers, getting\n",
    "## $$ 0 = \\det \\begin{pmatrix} \\lambda & -\\frac{2}{3} & -\\frac{2}{5} \\\\ -\\frac{1}{2} & \\lambda & -\\frac{3}{5} \\\\ -\\frac{1}{2} & -\\frac{1}{3} & \\lambda  \\end{pmatrix} $$\n",
    "### And writing out the determinant gives\n",
    "## $$ 0 = \\lambda^3-\\frac{1}{5}-\\frac{1}{15}-\\frac{1}{5}\\lambda-\\frac{1}{3}\\lambda-\\frac{1}{5}\\lambda $$\n",
    "### Simplifying we see that\n",
    "## $$ 0 = \\lambda^3-\\frac{11}{15}\\lambda-\\frac{4}{15} $$\n",
    "### Note that immediately we can see that $\\lambda_1=1$ is an eigenvalue, and we can rewrite the equation as\n",
    "## $$ 0 = (\\lambda-1)(\\lambda^2+\\lambda+\\frac{4}{15}) $$\n",
    "### We note that the second term has no real roots, so we only have one real eigenvalue. These complex eigenvalues have real part −1/2, which is less than the eigenvalue of 1, so this is the maximal eigenvalue. To now find the eigenvector corresponding to eigenvalue 1, we need to find a vector in the nullspace of\n",
    "## $$ \\begin{pmatrix} 1 & -\\frac{2}{3} & -\\frac{2}{5} \\\\ -\\frac{1}{2} & 1 & -\\frac{3}{5} \\\\ -\\frac{1}{2} & -\\frac{1}{3} & 1  \\end{pmatrix} $$\n",
    "### We now proceed with Gauss–Jordan elimination, first eliminating the 2nd and 3rd entries in the first column:\n",
    "## $$ \\begin{pmatrix} 1 & -\\frac{2}{3} & -\\frac{2}{5} \\\\ 0 & \\frac{2}{3} & -\\frac{4}{5} \\\\ 0 & -\\frac{2}{3} & \\frac{4}{5} \\end{pmatrix} $$\n",
    "### Now using the second row to eliminate the first nonzero term in the third, along with zeroing out the second term in the first row, we get\n",
    "## $$ \\begin{pmatrix} 1 & 0 & -\\frac{6}{5} \\\\ 0 & 1 & -\\frac{6}{5} \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n",
    "### We can immediately see that the vector\n",
    "## $$ \\bf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{5}{6} \\end{pmatrix} $$\n",
    "### is an eigenvector of $\\bf{M}$. Note that because we normalized by the number of links on each page, the many additional links on Buzzfeed make a much smaller contribution, and the rankings of Wikipedia and Facebook are much closer to that of Buzzfeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a593a1-0f42-4fea-a353-04378ca869bd",
   "metadata": {},
   "source": [
    "### Meaning of algoritm\n",
    "### It might have seemed interesting to you that one of the eigenvalues for the matrix was one. This is not a coincidence. When we normalized the matrix so that each column added to one, we were essentially giving the likelihood that someone on a given webpage would then go to any other webpage. If we imagine a large number of people surfing the internet, then at any given time they will each be occupying a particular page, and the people will migrate to other pages proportional to the number of links from their current page to their new page. Eventually in some sense we might expect a steady state to arise, where the number of people on any given page is constant, but people are constantly coming and going. If we have a vector represent the number of people at each website at one time, then multiplying it by our matrix corresponds to the number of people at each website after everyone clicks one link. In this case, the steady state corresponds to an eigenvector of people on each page that has an eigenvalue of one, meaning the distribution of people is the same after everyone clicks a link. Thus the page rank algorithm gives us the distribution of people on the internet if everyone is randomly surfing around, and captures the fact that a popular page will tend to have more people hanging out on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824aafc6-7e5e-4112-b926-a2a78d6e8a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
